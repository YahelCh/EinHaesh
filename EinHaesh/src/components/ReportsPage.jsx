import React, { useState, useRef } from 'react';
import './ReportsPage.css';

const ReportsPage = ({ location }) => {
  const [currentReport, setCurrentReport] = useState('');
  const [reports, setReports] = useState([]);
  const [recording, setRecording] = useState(false);
  const mediaRecorderRef = useRef(null);
  const audioChunksRef = useRef([]);
  const recognitionRef = useRef(null);
  const [currentTranscript, setCurrentTranscript] = useState(''); // ××©×ª× ×” ×œ×©××™×¨×ª ×”×ª××œ×•×œ ×‘×–××Ÿ ×××ª


  // ×¤×•× ×§×¦×™×” ×©××ª×—×™×œ×” ××ª ×”×”×§×œ×˜×”
  const startRecording = () => {
    setRecording(true);
    audioChunksRef.current = [];
    setCurrentTranscript(''); // ××™×¤×•×¡ ×”×ª××œ×•×œ ×œ×¤× ×™ ×›×œ ×”×§×œ×˜×” ×—×“×©×”
  
    let finalTranscript = ''; // ×”×’×“×¨×ª ××©×ª× ×” ×—×™×¦×•× ×™ ×œ×ª××œ×•×œ ×”×¡×•×¤×™
  
    navigator.mediaDevices
      .getUserMedia({ audio: true })
      .then((stream) => {
        mediaRecorderRef.current = new MediaRecorder(stream);
  
        // ××ª×—×™×œ ××ª ×”×ª××œ×•×œ ×‘×¨×’×¢ ×©×”×”×§×œ×˜×” ××ª×—×™×œ×”
        recognitionRef.current = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
        const recognition = recognitionRef.current;
  
        recognition.lang = 'he-IL';
        recognition.interimResults = true; // ×¢×“×›×•×Ÿ ×ª××œ×•×œ ×‘×–××Ÿ ×××ª
  
        recognition.onstart = () => {
          console.log('×”×ª×—×œ ×ª××œ×•×œ');
        };
  
        recognition.onresult = (event) => {
          // ×¨×§ ×ª××œ×•×œ×™× ×¡×•×¤×™×™× ×™×ª×•×•×¡×¤×•
          for (let i = event.resultIndex; i < event.results.length; i++) {
            if (event.results[i].isFinal) { // ×¨×§ ×× ××“×•×‘×¨ ×‘×ª××œ×•×œ ×¡×•×¤×™
              finalTranscript += event.results[i][0].transcript; // ×”×•×¡×¤×ª ×ª××œ×•×œ ×¡×•×¤×™ ×œ×ª×•×š finalTranscript
            }
          }
  
          console.log('×ª××œ×•×œ ×¢×“×›× ×™: ', finalTranscript);
          setCurrentTranscript(finalTranscript); // ×¢×“×›×•×Ÿ state ×¢× ×”×ª××œ×•×œ ×”× ×•×›×—×™
        };
  
        recognition.onerror = (event) => {
          console.error('×©×’×™××” ×‘×ª××œ×•×œ:', event.error);
        };
  
        recognition.onend = () => {
          console.log('×”×”×›×¨×” ×”×¡×ª×™×™××”');
          console.log("finalTranscript: ", finalTranscript); // ×”×“×¤×¡×ª ×”×ª××œ×•×œ ×”×¡×•×¤×™
  
          // ×©×•×œ×— ××ª ×”×ª××œ×•×œ ×× ×™×©
          if (finalTranscript.trim() !== '') {
            sendTranscription(finalTranscript); // ×©×œ×™×—×” ×©×œ ×”×ª××œ×•×œ
            setCurrentTranscript(finalTranscript); // ×¢×“×›×•×Ÿ state ×¢× ×”×ª××œ×•×œ ×”×¡×•×¤×™
          }
        };
  
        recognition.start();
  
        mediaRecorderRef.current.ondataavailable = (event) => {
          audioChunksRef.current.push(event.data);
        };
  
        mediaRecorderRef.current.onstop = () => {
          const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/wav' });
          const audioUrl = URL.createObjectURL(audioBlob);
  
          // ×”×•×¡×¤×ª ×”×”×§×œ×˜×” ×œ×¨×©×™××ª ×”×“×™×•×•×—×™× (×‘×œ×™ ×ª××œ×•×œ ×‘×”×ª×—×œ×”)
          const newReport = { id: Date.now(), audioUrl, transcript: '', isRecording: false };
          setReports((prevReports) => [...prevReports, newReport]);
  
          // ×¡×™×•× ×”×”×§×œ×˜×”
          recognition.stop(); // ×¡×™×•× ×”×ª××œ×•×œ
        };
  
        mediaRecorderRef.current.start();
      })
      .catch((err) => {
        console.error('Error accessing microphone:', err);
      });
  };
  

  const stopRecording = () => {
    if (mediaRecorderRef.current) {
      mediaRecorderRef.current.stop();
      setRecording(false);
    }
  };

  const sendTranscription = (transcript) => {
    setReports((prevReports) => {
      const updatedReports = [...prevReports];
      const lastReportIndex = updatedReports.length - 1; // × ×™×§×— ××ª ×”×“×™×•×•×— ×”××—×¨×•×Ÿ
  
      if (lastReportIndex >= 0) {
        updatedReports[lastReportIndex].transcript = transcript; // ×¢×“×›×•×Ÿ ×”×ª××œ×•×œ ×œ×“×™×•×•×— ×”××—×¨×•×Ÿ
        updatedReports[lastReportIndex].text = `×ª××œ×•×œ ×”×”×§×œ×˜×”: ${transcript}`; // ×”×•×¡×¤×ª ×”×ª××œ×•×œ ×œ×”×•×“×¢×”
        updatedReports[lastReportIndex].isTranscription = true; // ×”×’×“×¨×ª ×¡×•×’ ×”×”×•×“×¢×” ×œ×ª××œ×•×œ
      }
  
      return updatedReports;
    });
  };
  

  const addReport = () => {
    if (currentReport.trim() !== '') {
      setReports((prevReports) => [
        ...prevReports,
        { id: Date.now(), text: currentReport, isRecording: false },
      ]);
      setCurrentReport(''); // × ×§×” ××ª ×”×©×“×” ××—×¨×™ ×©×œ×™×—×”
    }
  };

  return (
    <div className="report-page">
      <div className="reports-title">×¢×“×›×•× ×™×</div>
      <div className="reports-list">
        {reports.map((report) => (
          <React.Fragment key={report.id}>
            {/* ×”×¦×’×ª ×”×”×§×œ×˜×” */}
            {report.audioUrl && (
              <div>
                <audio controls src={report.audioUrl}></audio>
              </div>
            )}
            {/* ×”×¦×’×ª ×˜×§×¡×˜ ×× ×§×™×™× */}
            {report.text && (
       <div className={`chat-bubble ${report.isTranscription ? 'transcription' : ''}`}>
       <p>{report.text}</p>
     </div>
     
            )}
          </React.Fragment>
        ))}
      </div>

      <div className="input-bar">
        <textarea
          value={currentReport}
          onChange={(e) => setCurrentReport(e.target.value)}
          placeholder="Type a message"
        />
        {currentReport && (
          <button onClick={addReport} className="send-button">
            â¤
          </button>
        )}
        <button
          onClick={recording ? stopRecording : startRecording}
          className={`record-button ${recording ? 'active' : ''}`}
        >
          {recording ? 'â¹' : 'ğŸ™ï¸'}
        </button>
      </div>
    </div>
  );
};

export default ReportsPage;
